---
title: "ejercicios de regresion en R"
author: "geovanni Jesus Ramírez"
date: "2022-12-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Se plantaron 8 pinos de 0.3 metros de altura en medios controlados y se los sometió a distintas intensidades de irrigación para simular el efecto de las diferentes precipitaciones pluviales. Al acabar el año se midieron las alturas. En la tabla siguiente se muestran las alturas medias (en metros)  (y_i) al acabar el año y la cantidad de lluvia (en metros) simulada por cada valor x_i. Suponemos que Y, la altura del árbol al acabar el año, es una variable  aleatoria con media beta_0+ beta_1 x, donde x es la precipitación, y con varianza constante  sigma^2 por todo valor de x. Hallar las mejores estimaciones lineales sin sesgo de  beta_0 y beta_1 y hallar una estimación sin sesgo de sigma^2.

     y_i        x_i

0.4826   0.2540

0.5588   0.3556

0.6350  0.4572

0.7874  0.5588

0.8382  0.6604

0.9906  0.7620

1.1176  0.8636

1.1430  0.9652

a) Estimar los valores $b_0$ y $b_1$ para la regresión lineal de la altura del pino en función de la cantidad de lluvia.

```{r}
y_i <- c(0.4826, 0.5588, 0.6350, 0.7874, 0.8382, 0.9906, 1.1176, 1.1430)
x_i <- c(0.2540, 0.3556, 0.4572, 0.5588, 0.6604, 0.7620, 0.8636, 0.9652)

df <-data.frame(y_i, x_i)
```


b) Representa gráficamente los datos
junto con la recta de regresión.

```{r}
reg <-lm(y_i~x_i, data = df)
reg
```



c) Hallar un intervalo de confianza al 95% de confianza para los parámetros beta_0 y beta_1.
```{r}
confint(lm(y_i~x_i, data=df))
```


d) Calcular la estimación de la varianza común de los errores de la regresión sigma^2.
```{r}
library(car)
library(nortest)
library(lmtest)
```

```{r}
SSE <- lm(y_i~x_i, data = df)$residuals
sum(SSE^2)/(length(x_i)-2)
```


e) Hallar el coeficiente de regresión y el coeficiente de regresión ajustado.
```{r}
summary(reg)$r.squared
summary(reg)$adj.r.squared
```


f) Estudiar si el modelo es homocedástico gráficamente y usando el test correspondiente.

```{r}
b0 <- reg$coefficients[1]
b1 <- reg$coefficients[2]
y_est <- b0 + b1*x_i
```

```{r}
plot(y_est, SSE, pch=25, col="Orange")
smoothingSpline = smooth.spline(y_est, SSE, spar=1)
lines(smoothingSpline, col='red', lwd=3)
```

```{r}
bptest(lm(y_i~x_i + I(x_i^2)))

bptest(reg)
```



g) Estudiar la normalidad de los residuos.

```{r}
lillie.test(SSE)
```


h) Estudiar la correlación de los residuos.
```{r}
dwtest(reg, alternative = "greater")

dwt(reg)
```


i) Hallar las observaciones "outliers", los "leverages" y las observaciones influyentes.

##leverages

```{r}
hat_values <- hatvalues(reg)

which(hat_values>(2*(1+1)/length(x_i)))

```
## outliers 

```{r}
outlierTest(reg)
```
leverages

```{r}
cooks.distance(reg)
```




Los siguientes datos relacionan la producción de biomasa de soja con la radiación solar interceptada acumulada durante un período de ocho semanas después de la emergencia. La producción de biomasa es el peso seco medio en gramos de muestras independientes de cuatro plantas.

X (Radiación solar)              Y Biomasa de la planta

29.7                                         16.6

68.4                                         49.1

120.7                                       121.7

217.2                                       219.6

313.5                                        375.5

419.1                                       570.8

535.9                                        648.2

641.5                                         755.6

a) Estimar los valores b_0 y b_1 para la regresión lineal de la biomasa de la planta en función de la radiación solar.   

```{r}
rad_solar <- c(29.7, 68.4, 120.7, 217.2, 313.5, 419.1, 535.9, 641.5)
biomasa <- c(16.6, 49.1, 121.7, 219.6, 375.5, 570.8, 648.2, 755.6)

df2 <- data.frame(biomasa, rad_solar)
```


b) Representa gráficamente los datos junto con la recta de regresión.       
```{r}
plot(rad_solar, biomasa, pch=20, col="green", lwd=1)
smoothingSpline = smooth.spline(rad_solar, biomasa, spar=1)
lines(smoothingSpline, col='blue', lwd=3)
```

```{r}
r <- lm(biomasa~rad_solar, data = df2)
r
```

c) Hallar un intervalo de confianza al 95% de confianza para los parámetros beta_0 y beta_1.
```{r}
confint(r)
```


d) Calcular la estimación de la varianza común de los errores de la regresión sigma^2.
```{r}
SSe <- lm(biomasa~rad_solar, data = df2)$residuals
sum(SSe^2)/(length(biomasa)-2)
```


e) Hallar el coeficiente de regresión y el coeficiente de regresión ajustado.   
```{r}
summary(r)$r.squared
summary(r)$adj.r.squared
```

f) Estudiar si el modelo es homocedástico gráficamente y usando el test correspondiente.

```{r}
y_est2 <- r$fitted.values
plot(y_est2, SSe, pch=20, col="green", lwd=1)
smoothingSpline = smooth.spline(y_est2, SSe, spar=1)
lines(smoothingSpline, col='blue', lwd=3)

```



```{r}
bptest(lm(biomasa~rad_solar+I(rad_solar^2)))
bptest(r)
```


g) Estudiar la normalidad de los residuos.                            
```{r}
lillie.test(SSe)
```

```{r}
dx <- density(SSe)
hist(SSe)
lines(dx, lwd=2, col="red")
plot(dx, lwd = 2, col = "red",
     main = "Densidad")


```


h) Estudiar la correlación de los residuos. 

```{r}
dwtest(r, alternative = "greater")
dwt(r)
```


i) Hallar las observaciones "outliers", los "leverages" y las observaciones influyentes.   



###leverages 

```{r}
hatval <- hatvalues(r)
which(hatval>(2*(1+1)/8))
```


####outliers

```{r}
outlierTest(r)
```

### ovservaciones influyentes

```{r}
dc <- cooks.distance(r)
which(dc > (4/(length(biomasa)-1-1)))
```


Se probó un modelo de simulación para el flujo máximo de agua de las cuencas hidrográficas comparando el flujo máximo medido de 10 tormentas con predicciones del flujo máximo obtenido del modelo de simulación. Q_o y Q_p son los flujos máximos observados y pronosticados, respectivamente. Se registraron cuatro variables independientes:

    - X_1: area de la cuenca (m^2),

    - X_2: pendiente promedio de la cuenca (en porcentaje),

    - X_3: índice de absorbencia superficial (0 = absorbencia completa, 100 = sin absorbencia), y

    - X_4: intensidad de pico de lluvia calculada en intervalos de media hora.



Q_o              Q_p             X_1             X_2             X_3             X_4

28              32              .03             3.0             70              .6

112             142             .03             3.0             80              1.8

398             502             .13             6.5             65              2.0

772             790             1.00            15.0            60              .4

2294            3075            1.00            15.0            65              2.3

2484            3230            3.00            7.0             67              1.0

2586            3535            5.00            6.0             62              .9

3024            4265            7.00            6.5             56              1.1

4179            6529            7.00            6.5             56              1.4

710             935             7.00            6.5             56              .7



Consideramos Y=ln(Q_o/Q_p) como variable dependiente, consideramos la regresión de Y como función de X_1, X_2, X_3 y X_4. Se pide:

```{r}
Q_o <- c(28, 112, 398, 772, 2294, 2484, 2586, 3024, 4179, 710)
x_4 <- c(.6, 1.8, 2.0, .4, 2.3, 1.0, .9, 1.1, 1.4, .7)
Q_p <- c(32, 142, 502, 790, 3075, 3230, 3553, 4265, 6529, 935)
x_1 <- c(.03, .03, .13, 1, 1, 3, 5, 7, 7, 7)
x_2 <- c(3, 3, 6.5, 15, 15, 7, 6, 6.5, 6.5, 6.5)
x_3 <- c(.6, 1.8, 2, .4, 2.3, 1, .9, 1.1, 1.4, 1.7)
yp <-log(Q_o/Q_p)
df3 <- data.frame(yp, x_1, x_2, x_3, x_4)

```


a) Estimar los valores b_0, b_1, b_2, b_3, b_4 para la regresión lineal de Y en función de X_i, i=1,2,3,4.
```{r}
reg2 <- lm(yp~ x_1 + x_2 + x_3 +x_4)
reg2
```


b) Hallar un intervalo de confianza al 95% de confianza para los parámetros beta_i, i=0,1,2,3,4.

```{r}
confint(reg2)
```


c) Calcular la estimación de la varianza común de los errores de la regresión sigma^2.

```{r}
sse <- reg2$residuals
sum(sse^2)/(length(x_4)-2)
```


d) Hallar el coeficiente de regresión y el coeficiente de regresión ajustado.
```{r}
summary(reg2)$r.squared
summary(reg2)$adj.r.squared
```


e) Estudiar si el modelo es homocedástico gráficamente y usando el test correspondiente.

```{r}
y_est3 <- reg2$fitted.values
plot(y_est3, sse, pch=20, col="green", lwd=1)
smoothingSpline = smooth.spline(y_est3, sse, spar=2)
lines(smoothingSpline, col='blue', lwd=3)
```


f) Estudiar la normalidad de los residuos.
```{r}
lillie.test(sse)
```


```{r}
dx <- density(sse)
hist(sse)
lines(dx, lwd=2, col="red")
plot(dx, lwd = 2, col = "red",
     main = "Densidad")
```


g) Estudiar la correlación de los residuos.
```{r}
dwtest(reg2, alternative="greater")
dwt(reg2)

```


h) Contrastar la linealidad y la aditividad del modelo.
```{r}
valores_ajustados <- y_est3^2

summary(lm(yp~x_1+x_2+x_3+x_4+valores_ajustados))[4]
```

```{r}
residualPlots(reg2)
```

#### linealidad de los modelos

```{r}
crPlots(reg2)
```




i) Hallar las observaciones "outliers", los "leverages" y las observaciones influyentes.

### leverages

```{r}
valores_hat <- hatvalues(reg2)
valores_hat

which(valores_hat > 2*(4+1)/length(x_1))
```

###outlier test
```{r}
outlierTest(reg2)
```


#### observaciones influyentes

```{r}
distancia <- cooks.distance(reg2)

which(distancia > 4/(length(x_1)-4-1))
```






